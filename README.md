# ğŸ“Š Data Mining Project - UCI Adult Dataset

Un proiect complet de **Data Mining** care analizeazÄƒ dataset-ul Adult (Census Income) de la UCI Machine Learning Repository. Acest README explicÄƒ fiecare concept È™i metodÄƒ folositÄƒ, presupunÃ¢nd cÄƒ cititorul nu are experienÈ›Äƒ anterioarÄƒ Ã®n data mining.

---

## ğŸ“– Ce este Data Mining?

**Data Mining** (sau "mineritul datelor") este procesul de descoperire a patternurilor, corelaÈ›iilor È™i informaÈ›iilor utile din seturi mari de date. GÃ¢ndeÈ™te-te la el ca la un detectiv digital care cautÄƒ indicii ascunse Ã®n munÈ›i de date.

### De ce este important?
- **Business**: PredicÈ›ia comportamentului clienÈ›ilor
- **MedicinÄƒ**: Detectarea bolilor din simptome
- **FinanÈ›e**: Detectarea fraudelor
- **Marketing**: Segmentarea clienÈ›ilor

---

## ğŸ“ Dataset-ul Adult (Census Income)

### Ce conÈ›ine?
Dataset-ul conÈ›ine informaÈ›ii din recensÄƒmÃ¢ntul SUA din 1994, cu scopul de a prezice dacÄƒ o persoanÄƒ cÃ¢È™tigÄƒ **mai mult de $50,000/an** sau nu.

| Atribut | Tip | Descriere |
|---------|-----|-----------|
| `age` | Numeric | VÃ¢rsta persoanei |
| `workclass` | Categoric | Tipul angajatorului (Private, Gov, Self-employed, etc.) |
| `fnlwgt` | Numeric | Pondere finalÄƒ (factor de reprezentativitate) |
| `education` | Categoric | Nivel de educaÈ›ie (Bachelors, Masters, etc.) |
| `education-num` | Numeric | NumÄƒr de ani de educaÈ›ie |
| `marital-status` | Categoric | Stare civilÄƒ |
| `occupation` | Categoric | OcupaÈ›ie |
| `relationship` | Categoric | RelaÈ›ie Ã®n familie |
| `race` | Categoric | RasÄƒ |
| `sex` | Categoric | Sex |
| `capital-gain` | Numeric | CÃ¢È™tiguri din capital |
| `capital-loss` | Numeric | Pierderi din capital |
| `hours-per-week` | Numeric | Ore lucrate pe sÄƒptÄƒmÃ¢nÄƒ |
| `native-country` | Categoric | Èšara de origine |
| `income` | **Target** | `>50K` sau `<=50K` (ce vrem sÄƒ prezicem) |

### Statistici
- **Total instanÈ›e**: 48,842
- **Training**: 32,561 instanÈ›e
- **Test**: 16,281 instanÈ›e
- **Missing values**: Marcate cu `?`

---

## ğŸ—ï¸ Structura Proiectului

```
DataMining/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ adult.data          # Date de antrenament
â”‚   â””â”€â”€ adult.test          # Date de test
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py         # IniÈ›ializare pachet Python
â”‚   â”œâ”€â”€ preprocessing.py    # PregÄƒtirea datelor
â”‚   â”œâ”€â”€ classification.py   # Task 1: Clasificare
â”‚   â”œâ”€â”€ outlier_detection.py# Task 2: Detectare outlieri
â”‚   â”œâ”€â”€ clustering.py       # Task 3: Clustering
â”‚   â”œâ”€â”€ association_rules.py# Task 4: Reguli de asociere
â”‚   â”œâ”€â”€ feature_selection.py# Task 5: SelecÈ›ia features
â”‚   â””â”€â”€ utils.py            # FuncÈ›ii utilitare
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ models/             # Modele antrenate salvate
â”‚   â”œâ”€â”€ plots/              # Grafice generate
â”‚   â””â”€â”€ reports/            # Rapoarte CSV
â”œâ”€â”€ main.py                 # Script principal
â”œâ”€â”€ requirements.txt        # DependenÈ›e Python
â””â”€â”€ README.md               # Acest fiÈ™ier
```

---

## ğŸ”§ Preprocesarea Datelor (`preprocessing.py`)

Ãnainte de a aplica orice algoritm, datele trebuie **pregÄƒtite**. Datele "brute" rareori sunt gata de utilizare.

### 1. Tratarea Valorilor LipsÄƒ
```
ProblemÄƒ: Unele cÃ¢mpuri au valoarea "?" (nu se È™tie)
SoluÈ›ie: Ãnlocuim cu cea mai frecventÄƒ valoare (modul)
```

**De ce?** Algoritmii nu pot procesa valori lipsÄƒ. Modalul (valoarea cea mai frecventÄƒ) este o aproximare sigurÄƒ.

### 2. Encoding-ul Variabilelor Categorice

**ProblemÄƒ**: Algoritmii Ã®nÈ›eleg doar numere, nu text ca "Private" sau "Bachelors".

**SoluÈ›ie**: **One-Hot Encoding**
```
occupation=Tech-support  â†’  [1, 0, 0, 0, ...]
occupation=Sales         â†’  [0, 1, 0, 0, ...]
occupation=Exec-manager  â†’  [0, 0, 1, 0, ...]
```

Fiecare categorie devine o coloanÄƒ separatÄƒ cu valori 0 sau 1.

### 3. Scalarea Variabilelor Numerice

**ProblemÄƒ**: Variabilele au scale diferite (age: 17-90, capital-gain: 0-99,999).

**SoluÈ›ie**: **StandardScaler** - transformÄƒ fiecare variabilÄƒ sÄƒ aibÄƒ:
- Media = 0
- DeviaÈ›ia standard = 1

**De ce?** Algoritmii sunt sensibili la scale. FÄƒrÄƒ scalare, `capital-gain` ar domina toate deciziile doar pentru cÄƒ are valori mai mari.

### 4. Feature Engineering

CreÄƒm **variabile noi** din cele existente pentru a Ã®mbunÄƒtÄƒÈ›i predicÈ›iile:

| Feature Nou | Formula | IntuiÈ›ie |
|-------------|---------|----------|
| `gain_loss_ratio` | capital-gain / (capital-loss + 1) | Raportul cÃ¢È™tig/pierdere |
| `is_high_hours` | 1 dacÄƒ hours > 40, altfel 0 | LucreazÄƒ overtime? |
| `education_efficiency` | education-num / age | CÃ¢t de repede a avansat Ã®n educaÈ›ie |

---

## ğŸ“‹ Task 1: Clasificare (`classification.py`)

### Ce este Clasificarea?
**Clasificarea** este procesul de a prezice o **categorie** (clasÄƒ) pentru date noi. Ãn cazul nostru: va cÃ¢È™tiga persoana >$50K sau nu?

### Algoritmii FolosiÈ›i

#### 1. Decision Tree (Arbore de Decizie)
```
                    [Age > 30?]
                   /           \
                 Yes            No
                /                 \
        [Education > 12?]      [Income: <=50K]
           /        \
         Yes         No
          |           |
    [Income: >50K]  [Hours > 40?]
                      /     \
                    Yes      No
                     |        |
              [>50K]      [<=50K]
```

**Cum funcÈ›ioneazÄƒ?** Ia decizii secvenÈ›iale bazate pe Ã®ntrebÄƒri simple. E ca un joc de "20 de Ã®ntrebÄƒri".

**Avantaje**: UÈ™or de interpretat, rapid
**Dezavantaje**: Poate "memora" datele (overfitting)

#### 2. Random Forest (PÄƒdure Aleatoare)
CreeazÄƒ **multe arbori de decizie** (ex: 100) È™i Ã®i lasÄƒ sÄƒ voteze.

```
Arbore 1: >50K     â”
Arbore 2: <=50K    â”‚
Arbore 3: >50K     â”œâ”€â”€â†’ Majoritatea: >50K  âœ“
Arbore 4: >50K     â”‚
Arbore 5: <=50K    â”˜
```

**De ce mai mulÈ›i arbori?** Un singur arbore poate greÈ™i. 100 de arbori care voteazÄƒ sunt mai stabili - "Ã®nÈ›elepciunea mulÈ›imii".

**Avantaje**: Foarte precis, rezistent la overfitting
**Dezavantaje**: Mai lent, mai greu de interpretat

#### 3. Logistic Regression (Regresie LogisticÄƒ)
Nu confunda cu regresia obiÈ™nuitÄƒ! CalculeazÄƒ **probabilitatea** de a aparÈ›ine unei clase.

```
P(income = >50K) = 1 / (1 + e^(-(wâ‚Â·age + wâ‚‚Â·education + ... + b)))
```

Rezultatul e Ã®ntre 0 È™i 1. DacÄƒ > 0.5, prezice >50K.

**Avantaje**: Rapid, oferÄƒ probabilitÄƒÈ›i, interpretabil
**Dezavantaje**: Presupune relaÈ›ii liniare

#### 4. XGBoost (Extreme Gradient Boosting)
Algoritmul "superstar" al competiÈ›iilor de ML. ConstruieÈ™te arbori **secvenÈ›ial**, fiecare corectÃ¢nd greÈ™elile precedentului.

```
Arbore 1: PredicÈ›ie iniÈ›ialÄƒ (slabÄƒ)
         â†“
Arbore 2: CorecteazÄƒ erorile arborelui 1
         â†“
Arbore 3: CorecteazÄƒ erorile rÄƒmase
         â†“
...
Final: SumÄƒ ponderatÄƒ a tuturor arborilor
```

**Avantaje**: Cel mai precis de obicei
**Dezavantaje**: Mai multe hiperparametri de reglat

### GridSearchCV - GÄƒsirea Celor Mai Buni Hiperparametri

**Hiperparametri** = setÄƒri ale algoritmului (ex: cÃ¢È›i arbori, cÃ¢t de adÃ¢nci).

**GridSearchCV** testeazÄƒ **toate combinaÈ›iile** È™i alege cea mai bunÄƒ:
```
max_depth: [5, 10, 15]
n_estimators: [50, 100]
                â†“
TesteazÄƒ: (5,50), (5,100), (10,50), (10,100), (15,50), (15,100)
                â†“
Alege combinaÈ›ia cu cel mai bun scor
```

### Metrici de Evaluare

| MetricÄƒ | Ce MÄƒsoarÄƒ | FormulÄƒ |
|---------|------------|---------|
| **Accuracy** | % predicÈ›ii corecte | (TP+TN) / Total |
| **Precision** | Din cei preziceÈ›i >50K, cÃ¢È›i chiar sunt | TP / (TP+FP) |
| **Recall** | Din cei care chiar sunt >50K, cÃ¢È›i am gÄƒsit | TP / (TP+FN) |
| **F1** | Media armonicÄƒ Precision-Recall | 2Â·PÂ·R / (P+R) |
| **ROC-AUC** | Capacitatea de a separa clasele | Aria sub curba ROC |

Unde: TP=True Positive, TN=True Negative, FP=False Positive, FN=False Negative

---

## ğŸ” Task 2: Detectarea Outlierilor (`outlier_detection.py`)

### Ce sunt Outlierii?
**Outlierii** (anomalii) sunt puncte de date care sunt **semnificativ diferite** de restul.

```
Date normale:    â— â— â— â— â— â— â— â—
Outlier:                           â˜… (departe de grup)
```

### De ce Ã®i cÄƒutÄƒm?
- Pot indica **fraude** (tranzacÈ›ii suspecte)
- Pot fi **erori** de introducere date
- Pot fi **cazuri rare** interesante (tineri foarte bogaÈ›i)

### Metoda 1: Isolation Forest
**Ideea**: Outlierii sunt mai uÈ™or de "izolat" (separat) decÃ¢t punctele normale.

```
Pas 1: Alege o variabilÄƒ random (ex: age)
Pas 2: Alege un prag random (ex: 35)
Pas 3: Ãmparte datele: age < 35 | age >= 35
Pas 4: RepetÄƒ pÃ¢nÄƒ izolezi fiecare punct

Outlierii: NecesitÄƒ PUÈšINE Ã®mpÄƒrÈ›iri pentru izolare
Normalii: NecesitÄƒ MULTE Ã®mpÄƒrÈ›iri
```

### Metoda 2: Local Outlier Factor (LOF)
**Ideea**: ComparÄƒ **densitatea localÄƒ** a unui punct cu vecinii sÄƒi.

```
Punct normal: Densitate similarÄƒ cu vecinii
Outlier: Densitate mult mai micÄƒ decÃ¢t vecinii (izolat)
```

LOF = Densitatea vecinilor / Densitatea punctului
- LOF â‰ˆ 1: Normal
- LOF >> 1: Outlier (vecinii sunt mai denÈ™i)

### Ce analizÄƒm?
FiltrÄƒm doar persoanele cu **income >50K** È™i cÄƒutÄƒm anomalii bazate pe:
- `age` (vÃ¢rstÄƒ)
- `hours-per-week` (ore lucrate)
- `capital-gain` (cÃ¢È™tiguri din investiÈ›ii)

**Exemplu de outlier gÄƒsit**: PersoanÄƒ de 22 ani, care lucreazÄƒ 99 ore/sÄƒptÄƒmÃ¢nÄƒ È™i are capital-gain de $99,999. Extrem de neobiÈ™nuit!

---

## ğŸ¯ Task 3: Clustering (`clustering.py`)

### Ce este Clustering-ul?
**Clustering** = gruparea datelor Ã®n categorii **fÄƒrÄƒ a È™ti dinainte** care sunt acele categorii. Algoritmul le descoperÄƒ singur.

```
Input: â— â— â— â—‹ â—‹ â—‹ â—‹ â–  â–  â–  â–  (puncte fÄƒrÄƒ etichete)
Output: Grup1(â—) Grup2(â—‹) Grup3(â– )
```

### DiferenÈ›a faÈ›Äƒ de Clasificare
- **Clasificare**: È˜tim categoriile (>50K sau <=50K), Ã®nvÄƒÈ›Äƒm sÄƒ le recunoaÈ™tem
- **Clustering**: NU È™tim categoriile, le descoperim

### Metoda 1: K-Means

**Algoritmul**:
```
1. Alege K centre aleatorii
2. Atribuie fiecare punct la centrul cel mai apropiat
3. RecalculeazÄƒ centrele (media punctelor din fiecare cluster)
4. RepetÄƒ paÈ™ii 2-3 pÃ¢nÄƒ nu se mai schimbÄƒ nimic
```

**Problema**: Trebuie sÄƒ alegem K (numÄƒrul de clustere).

**SoluÈ›ia 1: Elbow Method**
```
Inertia
   â”‚
   â”‚â•²
   â”‚ â•²
   â”‚  â•²___________  â† "cotul" - aici K e optim
   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ K
        2  3  4  5
```
Inertia = suma distanÈ›elor la centru. CÄƒutÄƒm "cotul" unde scÄƒderea Ã®ncetineÈ™te.

**SoluÈ›ia 2: Silhouette Score**
MÄƒsoarÄƒ cÃ¢t de bine separat e fiecare cluster. Scor Ã®ntre -1 È™i 1:
- 1 = clustere perfect separate
- 0 = clustere suprapuse
- -1 = puncte Ã®n clusterul greÈ™it

### Metoda 2: DBSCAN
**Density-Based Spatial Clustering of Applications with Noise**

**Ideea**: Clusterele sunt **zone dense** separate de zone goale.

```
Parametri:
- eps: Raza de cÄƒutare
- min_samples: Minimum puncte pentru o zonÄƒ densÄƒ

Tipuri de puncte:
â— Core point: Are â‰¥ min_samples vecini Ã®n raza eps
â—‹ Border point: Ãn raza unui core point, dar nu are destui vecini
âœ— Noise: Nici core, nici border (outlier)
```

**Avantaj**: DescoperÄƒ clustere de **orice formÄƒ** (nu doar sferice ca K-Means).

### PCA - Reducerea DimensionalitÄƒÈ›ii

**Problema**: Avem ~100 de variabile dupÄƒ encoding. Imposibil de vizualizat.

**SoluÈ›ia**: **Principal Component Analysis (PCA)** - comprimÄƒ datele pÄƒstrÃ¢nd informaÈ›ia importantÄƒ.

```
100 variabile â†’ PCA â†’ 5 componente (pÄƒstreazÄƒ ~60% din informaÈ›ie)
                   â†’ 2 componente (pentru vizualizare)
```

---

## ğŸ”— Task 4: Reguli de Asociere (`association_rules.py`)

### Ce sunt Regulile de Asociere?
DescoperÄƒ **relaÈ›ii de tipul "dacÄƒ X, atunci Y"** Ã®n date.

**Exemplul clasic**: Analiza coÈ™ului de cumpÄƒrÄƒturi
```
DacÄƒ client cumpÄƒrÄƒ pÃ¢ine È˜I unt â†’ probabil cumpÄƒrÄƒ È™i lapte
```

### Ãn proiectul nostru
```
DacÄƒ age=36-45 È˜I education=Bachelors È˜I hours=41-50 â†’ income=>50K
```

### Discretizarea
TransformÄƒm valorile numerice Ã®n categorii pentru a crea reguli:

| VariabilÄƒ | Bins |
|-----------|------|
| age | 0-25, 26-35, 36-45, 46-55, 56-100 |
| hours-per-week | 0-30, 31-40, 41-50, 51-100 |
| capital-gain | 0, 1-5000, 5001+ |

### Metrici pentru Reguli

| MetricÄƒ | FormulÄƒ | Interpretare |
|---------|---------|--------------|
| **Support** | P(A âˆ© B) | CÃ¢t de frecvent apare regula |
| **Confidence** | P(B\|A) = Support/P(A) | DacÄƒ A, cÃ¢t de probabil B? |
| **Lift** | Confidence / P(B) | De cÃ¢te ori mai probabil B cu A vs. fÄƒrÄƒ A |

**Lift > 1**: Asociere pozitivÄƒ (A creÈ™te È™ansa lui B)
**Lift = 1**: Independente
**Lift < 1**: Asociere negativÄƒ (A scade È™ansa lui B)

### Algoritmii

#### Apriori
```
1. GÄƒseÈ™te itemuri frecvente (support â‰¥ min_support)
2. CombinÄƒ-le Ã®n perechi, pÄƒstreazÄƒ pe cele frecvente
3. CombinÄƒ Ã®n triplete, etc.
4. GenereazÄƒ reguli din itemset-urile frecvente
```

#### FP-Growth
Mai eficient ca Apriori - construieÈ™te un arbore compact al tranzacÈ›iilor È™i extrage regulile fÄƒrÄƒ a scana datele de mai multe ori.

---

## ğŸšï¸ Task 5: SelecÈ›ia Features (`feature_selection.py`)

### De ce selectÄƒm features?
- **Prea multe variabile** = risc de overfitting
- **Curse of dimensionality**: performanÈ›a scade cu prea multe dimensiuni
- **EficienÈ›Äƒ**: model mai rapid cu mai puÈ›ine variabile

### Metodele Comparate

#### 1. Chi-Square (Ï‡Â²)
**Ideea**: MÄƒsoarÄƒ dependenÈ›a dintre o variabilÄƒ È™i target.

```
Ãntrebare: Este distribuÈ›ia lui X diferitÄƒ Ã®ntre clase?
           (ex: distribuÈ›ia "education" e diferitÄƒ Ã®ntre >50K È™i <=50K?)

Ï‡Â² mare â†’ Variabila e relevantÄƒ
Ï‡Â² mic â†’ Variabila e probabil irelevantÄƒ
```

#### 2. Mutual Information
**Ideea**: CÃ¢t de multÄƒ informaÈ›ie despre target obÈ›inem din variabilÄƒ?

```
I(X; Y) = cÃ¢t de mult reduce incertitudinea Ã®n Y cunoaÈ™terea lui X

I(education; income) = mare (È™tiind educaÈ›ia, È™tim mai bine venitul)
I(fnlwgt; income) = mic (ponderea nu spune nimic despre venit)
```

#### 3. RFE (Recursive Feature Elimination)
**Ideea**: EliminÄƒ iterativ cele mai slabe variabile.

```
1. AntreneazÄƒ model cu TOATE variabilele
2. IdentificÄƒ variabila cea mai puÈ›in importantÄƒ
3. Elimin-o
4. RepetÄƒ pÃ¢nÄƒ rÄƒmÃ¢n K variabile
```

**Avantaj**: Ia Ã®n considerare interacÈ›iunile Ã®ntre variabile.
**Dezavantaj**: Lent (antreneazÄƒ modelul de multe ori).

### Experimentul
TestÄƒm fiecare metodÄƒ cu K = 5, 8, 10, 12 variabile È™i comparÄƒm accuracy-ul.

---

## ğŸš€ Cum sÄƒ Rulezi Proiectul

### 1. Instalare DependenÈ›e
```bash
cd /home/bogdan/PersonalProjects/DataMining
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Rulare

```bash
# Toate task-urile (dureazÄƒ ~30-60 minute)
python main.py

# Un singur task
python main.py --task 1   # Doar clasificare
python main.py --task 2   # Doar outlieri
python main.py --task 3   # Doar clustering
python main.py --task 4   # Doar reguli asociere
python main.py --task 5   # Doar feature selection

# Mai multe task-uri
python main.py --task 1,3,5
```

### 3. Rezultate

DupÄƒ rulare, gÄƒseÈ™ti:
- **`results/models/`** - Modele antrenate (.joblib)
- **`results/plots/`** - Grafice (.png)
- **`results/reports/`** - Tabele rezultate (.csv)

---

## ğŸ“Š Exemplu de Rezultate

### Clasificare (Task 1)
| Model | Accuracy | F1-Score |
|-------|----------|----------|
| RandomForest | 86.2% | 0.71 |
| XGBoost | 87.1% | 0.73 |
| LogisticRegression | 85.0% | 0.68 |
| DecisionTree | 81.5% | 0.62 |

### Outlieri (Task 2)
- **DetectaÈ›i**: ~10% din instanÈ›ele high-income sunt outlieri
- **Cel mai extrem**: 22 ani, 55 ore/sÄƒpt, $99,999 capital-gain

### Clustering (Task 3)
- **K optim**: 3-4 clustere (dupÄƒ silhouette score)
- **Cluster 1**: Tineri, educaÈ›ie medie, ore normale
- **Cluster 2**: Maturi, educaÈ›ie Ã®naltÄƒ, ore multe

---

## ğŸ“š Resurse pentru ÃnvÄƒÈ›are

1. **Scikit-learn Documentation**: https://scikit-learn.org/stable/
2. **Coursera - Machine Learning** (Andrew Ng)
3. **Kaggle Learn**: https://www.kaggle.com/learn
4. **UCI ML Repository**: https://archive.ics.uci.edu/

---

## ğŸ¤ Autor

Proiect creat pentru cursul de **Data Mining**.

---

*"In God we trust. All others must bring data."* â€” W. Edwards Deming
